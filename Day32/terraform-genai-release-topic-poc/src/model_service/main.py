# src/model_service/main.py
from fastapi import FastAPI
from pydantic import BaseModel, Field

from config.settings import get_settings

settings = get_settings()

app = FastAPI(
    title="Day32 GenAI Model Service (d32-release)",
    version="0.1.0",
    description=(
        "Model service for the Day32 PoC. "
        "Later, it will wrap a small OSS model and can also route to OpenAI."
    ),
)


class GenerateRequest(BaseModel):
    prompt: str = Field(
        ...,
        description="Prompt text to send to the OSS model.",
    )


class GenerateResponse(BaseModel):
    text: str = Field(..., description="Model-generated text.")
    model: str = Field(..., description="Model identifier.")


@app.post("/api/v1/generate", response_model=GenerateResponse)
def generate_text(body: GenerateRequest) -> GenerateResponse:
    """
    Minimal OSS text generation endpoint.

    This stub makes the `oss` provider path usable even without a real model.
    - If USE_MOCK_LLM=true, returns a deterministic mock string.
    - Otherwise, returns a simple echo-style response tagged with a faux model name.
    """
    cleaned_prompt = (body.prompt or "").strip()
    if settings.use_mock_llm:
        text = f"[oss-mock] Response for prompt: {cleaned_prompt[:300]}"
        model = "oss-mock"
    else:
        text = (
            "Release note summary: "
            f"{cleaned_prompt[:220]} ... (generated by OSS model stub)"
        )
        model = "oss-mini"

    return GenerateResponse(text=text, model=model)


@app.get("/health")
def health() -> dict:
    """
    Basic health endpoint for the Model Service.

    In later parts, this service will expose a /generate endpoint that:
    - Calls a local open-source model running in this pod, or
    - Forwards to OpenAI (or other providers),
    based on configuration and/or UI selection.
    """
    return {
        "status": "ok",
        "service": "model-service",
        "env": settings.app_env,
        "llm_default_provider": settings.llm_default_provider,
        "use_mock_llm": settings.use_mock_llm,
        "part": 4,
    }
