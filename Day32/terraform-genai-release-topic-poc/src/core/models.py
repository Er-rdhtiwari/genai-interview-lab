# src/core/models.py
from __future__ import annotations

from datetime import date
from enum import Enum
from typing import List, Optional

from pydantic import BaseModel, Field


# ------------------------
# Provider Enum
# ------------------------


class ModelProvider(str, Enum):
    """Supported LLM providers for this PoC."""

    OPENAI = "openai"
    OSS = "oss"


# ------------------------
# Release Notes Contracts
# ------------------------


class ReleaseNoteRequest(BaseModel):
    """
    User input for generating release notes + test scenarios.

    Used by:
    - Streamlit UI --> Release Notes API
    - Direct API clients (Postman, curl, etc.)
    """

    title: str = Field(
        ...,
        description="Short feature or bugfix title, shown in release notes.",
        example="Improve login error messages",
    )
    description: str = Field(
        ...,
        description="Free-form description of the change (what, why, how).",
        example="We clarified login error messages for wrong password vs locked account.",
    )
    risk_level: Optional[str] = Field(
        default=None,
        description="Optional risk level (low/medium/high).",
        example="low",
    )
    impact_area: Optional[str] = Field(
        default=None,
        description="Optional area impacted (UI, backend, infra, data, etc.).",
        example="authentication",
    )


class ReleaseNoteResponse(BaseModel):
    """
    AI-generated release notes + suggested test scenarios.

    Returned by:
    - Release Notes API to UI / clients.
    """

    release_note: str = Field(
        ...,
        description="Final release note text suitable for end users.",
    )
    test_scenarios: List[str] = Field(
        default_factory=list,
        description="List of test scenarios derived from the change description.",
    )
    provider: str = Field(
        ...,
        description="Provider used for generation (e.g., 'openai', 'oss').",
    )
    model: Optional[str] = Field(
        default=None,
        description="Model identifier if known (e.g., 'gpt-3.5-turbo').",
    )
    cached: bool = Field(
        default=False,
        description="True if result came from cache (Redis) instead of a fresh LLM call.",
    )


# ------------------------
# Greeting Contracts
# ------------------------


class GreetingRequest(BaseModel):
    """
    Input for the greeting feature used in the Streamlit UI.

    - name: name of the person.
    - date_of_birth: YYYY-MM-DD string parsed into a `date`.
    - provider: optional override to choose 'openai' or 'oss' explicitly.
    """

    name: str = Field(
        ...,
        description="Person's name to greet.",
        example="Radhe",
    )
    date_of_birth: date = Field(
        ...,
        description="Date of birth (YYYY-MM-DD). Month is used for birthday checks.",
        example="1995-01-15",
    )
    provider: Optional[ModelProvider] = Field(
        default=None,
        description=(
            "Optional override for the greeting provider. "
            "If omitted, the system uses the default from settings."
        ),
    )


class GreetingResponse(BaseModel):
    """
    Output of the greeting feature.

    - greeting_message: human-friendly text from LLM.
    - is_birthday_month: whether birth month matches current month.
    - provider/model: which backend produced the greeting.
    """

    greeting_message: str = Field(
        ...,
        description="The actual greeting text to show in the UI.",
    )
    is_birthday_month: bool = Field(
        ...,
        description="True if the person's birth month equals the current month.",
    )
    provider: str = Field(
        ...,
        description="Provider used to generate the greeting (e.g., 'openai', 'oss').",
    )
    model: Optional[str] = Field(
        default=None,
        description="Model identifier if known (e.g., 'gpt-3.5-turbo', 'tiny-oss-model').",
    )
    cached: bool = Field(
        default=False,
        description="True if the greeting came from cache (Redis).",
    )


# ------------------------
# LLM Internal Model
# ------------------------


class LLMGenerationResult(BaseModel):
    """
    Internal representation of an LLM call result.

    Used between:
    - LLM client and release-notes/greeting services.
    """

    text: str = Field(
        ...,
        description="Raw text generated by the LLM or mock.",
    )
    provider: str = Field(
        ...,
        description="Provider used (e.g., 'openai', 'oss', 'mock').",
    )
    model: Optional[str] = Field(
        default=None,
        description="Model identifier if available.",
    )


# ------------------------
# Health Contracts
# ------------------------


class HealthResponse(BaseModel):
    """
    Standard health response for our services.

    Used by:
    - Release Notes API /health
    - Model Service /health
    """

    status: str = Field(
        ...,
        description="Service status, usually 'ok' or 'error'.",
        example="ok",
    )
    service: str = Field(
        ...,
        description="Logical service name, e.g., 'release-notes-api' or 'model-service'.",
    )
    env: str = Field(
        ...,
        description="Environment label (local/dev/aws/etc.).",
        example="local",
    )
    llm_default_provider: str = Field(
        ...,
        description="Configured default provider from settings.",
        example="openai",
    )
    use_mock_llm: bool = Field(
        ...,
        description="Whether the service is currently in mock mode.",
    )
