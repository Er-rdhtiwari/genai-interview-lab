{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bcd166",
   "metadata": {},
   "source": [
    "# Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e764dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "EURI_API_KEY = os.getenv(\"EURI_API_KEY\")\n",
    "def generate_completion():\n",
    "    url = \"https://api.euron.one/api/v1/euri/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {EURI_API_KEY}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a poem about artificial intelligence\"\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"gpt-4.1-nano\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    data = response.json()\n",
    "    data = json.dumps(data, indent=2)\n",
    "    print(data)\n",
    "\n",
    "generate_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f4d51",
   "metadata": {},
   "source": [
    "# Text Embeddings & Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb47098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    url = \"https://api.euron.one/api/v1/euri/embeddings\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {EURI_API_KEY}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"input\": text,\n",
    "        \"model\": \"text-embedding-3-small\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    data = response.json()\n",
    "    \n",
    "    embedding = np.array(data['data'][0]['embedding'])\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "text = \"The weather is sunny today.\"\n",
    "\n",
    "embedding = generate_embeddings(text)\n",
    "print(f\"Embedding len: {len(embedding)} Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bbb6d",
   "metadata": {},
   "source": [
    "# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "# Requires sentence-transformers>=2.7.0\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n",
    "# together with setting `padding_side` to \"left\":\n",
    "# model = SentenceTransformer(\n",
    "#     \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n",
    "#     tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    "# )\n",
    "\n",
    "# The queries and documents to embed\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "# Encode the queries and documents. Note that queries benefit from using a prompt\n",
    "# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n",
    "# also pass your own prompt via the `prompt` argument\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
    "print(f\"Document embeddings shape: {document_embeddings.shape}\")\n",
    "\n",
    "# Compute the (cosine) similarity between the query and document embeddings\n",
    "similarity = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarity)\n",
    "# tensor([[0.7646, 0.1414],\n",
    "#         [0.1355, 0.6000]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa33cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class InMemoryVectorStore(VectorStore):\n",
    "    \"\"\"\n",
    "    Extremely simplified vector store for demos and tests.\n",
    "    Stores all vectors in a list and does a linear scan.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._docs: List[Document] = []\n",
    "        self._embeddings: List[List[float]] = []\n",
    "\n",
    "    def add_documents(self, docs: List[Document], embeddings: List[List[float]]) -> None:\n",
    "        assert len(docs) == len(embeddings), \"Docs and embeddings must align\"\n",
    "        self._docs.extend(docs)\n",
    "        self._embeddings.extend(embeddings)\n",
    "\n",
    "    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:\n",
    "        dot = sum(x * y for x, y in zip(a, b))\n",
    "        norm_a = math.sqrt(sum(x * x for x in a))\n",
    "        norm_b = math.sqrt(sum(y * y for y in b))\n",
    "        if norm_a == 0 or norm_b == 0:\n",
    "            return 0.0\n",
    "        return dot / (norm_a * norm_b)\n",
    "\n",
    "    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        scores: List[Tuple[Document, float]] = []\n",
    "        for doc, emb in zip(self._docs, self._embeddings):\n",
    "            score = self._cosine_similarity(query_embedding, emb)\n",
    "            scores.append((doc, score))\n",
    "\n",
    "        # sort by score descending and take top_k\n",
    "        scores.sort(key=lambda pair: pair[1], reverse=True)\n",
    "        return scores[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingProvider(ModelProvider):\n",
    "    def __init__(self, provider: ModelProvider, logger):\n",
    "        super().__init__(provider.config)   # keep same config\n",
    "        self._provider = provider           # wrapped provider (OpenAI, Anthropic, etc.)\n",
    "        self._logger = logger\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        self._logger.info(f\"[{self.config.name}] generate() called\")\n",
    "        result = self._provider.generate(prompt, **kwargs)   # delegate to real provider\n",
    "        self._logger.info(f\"[{self.config.name}] generate() finished\")\n",
    "        return result\n",
    "\n",
    "    def embed(self, texts: list[str]) -> list[list[float]]:\n",
    "        self._logger.info(f\"[{self.config.name}] embed() called with n={len(texts)}\")\n",
    "        vectors = self._provider.embed(texts)\n",
    "        self._logger.info(f\"[{self.config.name}] embed() finished\")\n",
    "        return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fddf6",
   "metadata": {},
   "source": [
    "# FAISS Vector Database Implementation:\n",
    "\n",
    "#FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9aecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f12134",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
